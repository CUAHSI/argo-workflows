apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  creationTimestamp: "2023-08-08T17:56:33Z"
  generateName: map-reduce-aorc-
  generation: 112
  labels:
    workflows.argoproj.io/creator: "112251210534458906530"
  name: map-reduce-aorc
  namespace: workflows
  resourceVersion: "256676871"
  uid: 6f76a23a-2c62-4d47-a3a7-e8a5f91a0cca
spec:
  arguments:
    parameters:
    - name: jobId
    - name: timestepsPerGroup
      value: "4"
    - name: startDateTime
      value: "2010-01-01 00:00:00"
    - name: endDateTime
      value: "2010-01-01 04:00:00"
    - name: shapefileBucket
      value: subsetter-outputs
    - name: shapefilePath
      value: q1w2e3r4t5y6u7i8o9p0
    - name: watershedFilename
      value: watershed.shp
  entrypoint: main
  templates:
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: timestepsPerGroup
            value: '{{workflow.parameters.timestepsPerGroup}}'
          - name: startDateTime
            value: '{{workflow.parameters.startDateTime}}'
          - name: endDateTime
            value: '{{workflow.parameters.endDateTime}}'
        name: split
        template: split
      - arguments:
          artifacts:
          - gcs:
              bucket: argo_artifacts_ciroh
              key: '{{workflow.name}}/parts/{{item}}.json'
              serviceAccountKeySecret:
                key: serviceAccountKey
                name: my-gcs-creds
            name: part
          parameters:
          - name: partId
            value: '{{item}}'
          - name: shapefile-bucket
            value: '{{workflow.parameters.shapefileBucket}}'
          - name: shapefile-path
            value: '{{workflow.parameters.shapefilePath}}'
          - name: watershed-filename
            value: '{{workflow.parameters.watershedFilename}}'
          - name: job-id
            value: '{{workflow.parameters.jobId}}'
        depends: split
        name: map
        template: map
        withParam: '{{tasks.split.outputs.result}}'
    inputs: {}
    metadata: {}
    name: main
    outputs: {}
  - inputs:
      parameters:
      - name: timestepsPerGroup
      - name: startDateTime
      - name: endDateTime
    metadata: {}
    name: split
    outputs:
      artifacts:
      - archive:
          none: {}
        gcs:
          bucket: argo_artifacts_ciroh
          key: '{{workflow.name}}/parts'
          serviceAccountKeySecret:
            key: serviceAccountKey
            name: my-gcs-creds
        name: parts
        path: /mnt/out
    script:
      command:
      - python
      image: pandas/pandas:pip-minimal
      name: Split-Script
      resources: {}
      source: "import json\nimport os\nimport sys\nfrom datetime import datetime,
        timedelta \nimport pip\npip.main(['install', '-q', 'pandas'])\nimport pandas\nos.mkdir(\"/mnt/out\")\nst
        = datetime.strptime('{{inputs.parameters.startDateTime}}', '%Y-%m-%d %H:%M:%S')\net
        = datetime.strptime('{{inputs.parameters.endDateTime}}', '%Y-%m-%d %H:%M:%S')\net_padded
        = et + timedelta(hours=1)\nbins = pandas.date_range(start=st, end=et, freq='{{inputs.parameters.timestepsPerGroup}}H').union([st,
        et_padded])\ninput_params = [] \nfor i in range(0, len(bins)-1):\n\n    et_reduced
        = bins[i+1] - timedelta(hours=1)\n\n    input_params.append({'start': bins[i].isoformat(),
        'end': et_reduced.isoformat()})\n\n\n# store the number of jobs required since
        this may differ from the \n# number of parts specified as input. \nrequired_jobs
        = []\nfor i in range(0, len(bins) - 1):\n    with open(f\"/mnt/out/{i}.json\",
        \"w\") as f:\n        json.dump(input_params[i], f, indent=4, sort_keys=True)\n
        \       required_jobs.append(i)\n\n# dump the parts to stdout.  # This is
        how they are passed to the map # template\njson.dump(required_jobs, sys.stdout)\n"
  - inputs:
      artifacts:
      - name: part
        path: /mnt/in/part.json
      - gcs:
          bucket: '{{inputs.parameters.shapefile-bucket}}'
          key: '{{inputs.parameters.shapefile-path}}/'
          serviceAccountKeySecret:
            key: serviceAccountKey
            name: my-gcs-creds
        name: shapefile-input
        path: /srv/shp-data
      parameters:
      - name: partId
      - name: shapefile-bucket
      - name: shapefile-path
      - name: watershed-filename
      - name: job-id
    metadata: {}
    name: map
    outputs:
      artifacts:
      - archive:
          none: {}
        gcs:
          bucket: subsetter-outputs
          key: aorc/{{inputs.parameters.job-id}}
          serviceAccountKeySecret:
            key: serviceAccountKey
            name: my-gcs-creds
        name: aorc-output
        path: /srv/output
    script:
      command:
      - python
      image: cuahsi/aorc:latest
      name: CollectAORC-Script
      ports:
      - containerPort: 8787
      resources:
        requests:
          cpu: "2"
          ephemeral-storage: 10Gi
          memory: 6Gi
      source: "import json \nimport os \nimport sys \nfrom datetime import datetime\nfrom
        dask.distributed import Client\n\n# make sure that entry.py is in the path.\n#
        I have no idea why this is necessary since we're already \n# in the /srv directory,
        but it won't work without it.\n\nsys.path.append('/srv')\nimport entry\n\nif
        __name__ == \"__main__\":\n  client = Client(n_workers=3, memory_limit='2GB')\n
        \ print(client)\n\n  os.mkdir(\"/mnt/out\")\n\n  # load the input parameters
        from the split function\n\n  with open(\"/mnt/in/part.json\") as f:\n    part
        = json.load(f)\n\n  print(part)\n\n  st = datetime.fromisoformat(part['start'])
        \n\n  et = datetime.fromisoformat(part['end'])\n\n  # run the aorc subsetting
        entry script\n  entry.main(st, et, '/srv/shp-data/{{inputs.parameters.watershed-filename}}',
        '/srv/output', verbose=True)\n\n  #entry.main(st, et, '/srv/shp-data/{{watershed-filename}}',\n
        \ #'/srv/output', num_workers=1, timesteps_per_group=10,\n  #worker_mem_limit=4,
        verbose=True)\n"
